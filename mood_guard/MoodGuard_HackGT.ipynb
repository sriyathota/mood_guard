{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sriyathota/mood_guard/blob/main/mood_guard/MoodGuard_HackGT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install tensorflow\n",
        "!pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIEwe3F4Fr59",
        "outputId": "b05b692d-7177-4f6f-ff37-e6042ec879df",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XXCC6MWKEXcM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af8160d3-d855-448c-a288-1b0d0a3ad132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Smv_0k_G5EU",
        "outputId": "536b4c44-6f61-4392-9e6b-8ffb336a4a1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "      Unnamed: 0                                          statement  \\\n",
            "0          14356  hey, i want to die but I hate physical pain. l...   \n",
            "1          18668  I am constantly in a state of anxiety. I canno...   \n",
            "2           2219                  nct who made my nervous comeback.   \n",
            "3          46039  Help: Explaining to my family Hello! I was rec...   \n",
            "4           8350  I am just done. I have accepted that I do not ...   \n",
            "...          ...                                                ...   \n",
            "9995        4053                                        QUE JYP QUE   \n",
            "9996       39565  did i give myself erp year ago i think i did w...   \n",
            "9997       28892  *Major trigger warning* I find comfort in conf...   \n",
            "9998       52526  Has anyone taken prozak? Has anyone taken proz...   \n",
            "9999       39445  hello i m having some trouble understanding my...   \n",
            "\n",
            "          status  \n",
            "0       Suicidal  \n",
            "1       Suicidal  \n",
            "2         Normal  \n",
            "3        Bipolar  \n",
            "4     Depression  \n",
            "...          ...  \n",
            "9995      Normal  \n",
            "9996  Depression  \n",
            "9997      Stress  \n",
            "9998     Anxiety  \n",
            "9999  Depression  \n",
            "\n",
            "[10000 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "##IMPORTS AND LOADING DATA IN\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import os\n",
        "import transformers as tf\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "file_path = '/content/drive/My Drive/new_sentiment_data.csv'\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "df = df[0:10000]\n",
        "\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DATA PREPROCESSING:\n",
        "\n",
        "#remove any rows that have missing values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "\n",
        "# Extract text and labels\n",
        "sentences = df['statement'].values\n",
        "labels = df['status'].values\n",
        "\n",
        "#split data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Function to clean text - make lowercase, remove punctuation\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "# Clean sentences\n",
        "df['cleaned_statements'] = df['statement'].apply(clean_text)\n",
        "\n",
        "# Split the dataset\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "\"\"\"\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    sentences, labels, test_size=0.2\n",
        ")\n",
        "\"\"\"\n",
        "#import TfidfVectorizer:\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#vectorization for first column (sentences)\n",
        "ct = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('tfidf', TfidfVectorizer(), 'cleaned_statements'),  # Use TF-IDF on the 'statement' column\n",
        "    ],\n",
        "    remainder='drop'  # Drop any columns not specified in the transformers\n",
        ")\n",
        "transformed_data = ct.fit_transform(df)\n",
        "print(transformed_data)\n",
        "\n",
        "#encoding second column(status)\n",
        "label_encoder = LabelEncoder()\n",
        "df['status_encoded'] = label_encoder.fit_transform(df['status'])\n",
        "print(df['status_encoded'])\n",
        "print(df.shape[0])"
      ],
      "metadata": {
        "id": "BCjUmcH6HpCA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44e645df-d75a-464e-e5a2-1e7ba2e2b104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 11221)\t0.11511417920459249\n",
            "  (0, 26774)\t0.053315452788430744\n",
            "  (0, 25021)\t0.18872258733549113\n",
            "  (0, 6456)\t0.08247287377306887\n",
            "  (0, 3407)\t0.04147184129684775\n",
            "  (0, 10875)\t0.08099683386135056\n",
            "  (0, 18274)\t0.2322800203901963\n",
            "  (0, 17640)\t0.2591506254717433\n",
            "  (0, 14085)\t0.16117804753038195\n",
            "  (0, 23469)\t0.135217600160362\n",
            "  (0, 7310)\t0.10191479350879139\n",
            "  (0, 25236)\t0.135217600160362\n",
            "  (0, 15721)\t0.09801609826469272\n",
            "  (0, 991)\t0.13662242283178544\n",
            "  (0, 10329)\t0.1577529000883943\n",
            "  (0, 21313)\t0.08776003955662313\n",
            "  (0, 6906)\t0.08787081010415079\n",
            "  (0, 27839)\t0.0548750287245449\n",
            "  (0, 10592)\t0.10956671077182843\n",
            "  (0, 13554)\t0.05374161619832288\n",
            "  (0, 16993)\t0.07640427490206061\n",
            "  (0, 1224)\t0.13434632216873463\n",
            "  (0, 26885)\t0.21575356947543733\n",
            "  (0, 3538)\t0.05642903950613428\n",
            "  (0, 9713)\t0.13348598335448306\n",
            "  :\t:\n",
            "  (9926, 11117)\t0.09311698629968618\n",
            "  (9926, 26394)\t0.2508157152429451\n",
            "  (9926, 11082)\t0.07976546113022485\n",
            "  (9926, 9721)\t0.10986157930043092\n",
            "  (9926, 9064)\t0.061509156206008005\n",
            "  (9926, 1519)\t0.06180114486997872\n",
            "  (9926, 25472)\t0.28834766342185464\n",
            "  (9926, 10196)\t0.07480596974563988\n",
            "  (9926, 1030)\t0.09611588780728487\n",
            "  (9926, 9572)\t0.13139090523552438\n",
            "  (9926, 10901)\t0.10749962408743645\n",
            "  (9926, 15780)\t0.14221547435989965\n",
            "  (9926, 7973)\t0.08802260098150437\n",
            "  (9926, 12016)\t0.10749962408743645\n",
            "  (9926, 20801)\t0.10460115964962174\n",
            "  (9926, 25352)\t0.10814230701801789\n",
            "  (9926, 19797)\t0.1113712663122645\n",
            "  (9926, 2198)\t0.08222870981013103\n",
            "  (9926, 8187)\t0.11098135203010454\n",
            "  (9926, 6812)\t0.13391441772189694\n",
            "  (9926, 23519)\t0.11636268126676617\n",
            "  (9926, 20528)\t0.12049922108518281\n",
            "  (9926, 26535)\t0.12787113797315688\n",
            "  (9926, 6150)\t0.14594083693262994\n",
            "  (9926, 4750)\t0.15796725614336293\n",
            "0       6\n",
            "1       6\n",
            "2       3\n",
            "3       1\n",
            "4       2\n",
            "       ..\n",
            "9995    3\n",
            "9996    2\n",
            "9997    5\n",
            "9998    0\n",
            "9999    2\n",
            "Name: status_encoded, Length: 9927, dtype: int64\n",
            "9927\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TOKENIZATION\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Extract the \"statement\" column from the DataFrame *****PUT IN CLEANED STATEMENS WHEN COMBINING CODE\n",
        "train_statements = train_df['statement'].astype(str).tolist()\n",
        "test_statements = test_df['statement'].astype(str).tolist()\n",
        "\n",
        "#BERT TOKENIZER INSTALLATION:\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# LOAD PRE-TRAINED MODEL:\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "#TOKENIZING TEXT DATA:\n",
        "train_inputs = tokenizer(train_statements,\n",
        "                   padding=True,           # Pads sequences to the same length\n",
        "                   truncation=True,         # Truncates long sequences\n",
        "                   max_length=512,          # Max length for BERT\n",
        "                   return_tensors='pt')     # Return PyTorch tensors\n",
        "print(train_inputs)\n",
        "test_inputs = tokenizer(test_statements,\n",
        "                   padding=True,           # Pads sequences to the same length\n",
        "                   truncation=True,         # Truncates long sequences\n",
        "                   max_length=512,          # Max length for BERT\n",
        "                   return_tensors='pt')     # Return PyTorch tensors\n",
        "\n",
        "\n",
        "# Extract tokenized inputs\n",
        "train_input_ids = train_inputs['input_ids']\n",
        "train_attention_mask = train_inputs['attention_mask']\n",
        "\n",
        "test_input_ids = test_inputs['input_ids']\n",
        "test_attention_mask = test_inputs['attention_mask']\n",
        "\n",
        "# Extracting unique labels from csv column \"status\"\n",
        "train_labels = train_df['status'].tolist()  # This will be a Numpy Array\n",
        "test_labels = test_df['status'].tolist()  # This will be a Numpy Array\n",
        "\n",
        "# Create a LabelEncoder instance\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit the encoder to the labels and transform them into numerical format\n",
        "encoded_train_labels = label_encoder.fit_transform(train_labels)  # This will convert each label to a unique integer\n",
        "encoded_test_labels = label_encoder.fit_transform(test_labels)\n",
        "\n",
        "\n",
        "\n",
        "#creates dictionary to hold connection between string label & integer assigned by LabelEncoder()\n",
        "label_mapping = {label: index for index, label in enumerate(label_encoder.classes_)}\n",
        "print(\"Label mapping:\", label_mapping)\n",
        "\n",
        "# Convert input_ids, attention_mask, and labels to torch tensors\n",
        "#TORCH TENSORS --> data structue (arrays/matrices rather than python lists) which can be fed into model\n",
        "train_input_ids_tensor = torch.tensor(train_input_ids, dtype=torch.long)\n",
        "train_attention_mask_tensor = torch.tensor(train_attention_mask, dtype=torch.long)\n",
        "train_labels_tensor = torch.tensor(encoded_train_labels, dtype=torch.long)\n",
        "\n",
        "test_input_ids_tensor = torch.tensor(test_input_ids, dtype=torch.long)\n",
        "test_attention_mask_tensor = torch.tensor(test_attention_mask, dtype=torch.long)\n",
        "test_labels_tensor = torch.tensor(encoded_test_labels, dtype=torch.long)\n",
        "\n",
        "# Create a TensorDataset\n",
        "train_dataset = TensorDataset(train_input_ids_tensor, train_attention_mask_tensor, train_labels_tensor)\n",
        "test_dataset = TensorDataset(test_input_ids_tensor, test_attention_mask_tensor, test_labels_tensor)\n",
        "\n",
        "# Create a DataLoader to load the data in batches\n",
        "batch_size = 16  # You can adjust this based on memory capacity\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "# Load pre-trained BERT with a classification layer for your task (adjust num_labels)\n",
        "num_labels = len(label_mapping)  # Number of unique labels in your dataset\n",
        "\n",
        "#BERT(pre-trained model): learns the patterns of statements to sentiments during training to later predict the sentiment of new statements\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fCOLTG3UHx35",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99546862-06df-4fec-b3a7-204481d14429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101, 14736,  5003,  ...,     0,     0,     0],\n",
            "        [  101, 23760,  3366,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  2069,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  1045,  2202,  ...,     0,     0,     0],\n",
            "        [  101,  3782,  9336,  ...,     0,     0,     0],\n",
            "        [  101,  4931,  2417,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
            "Label mapping: {'Anxiety': 0, 'Bipolar': 1, 'Depression': 2, 'Normal': 3, 'Personality disorder': 4, 'Stress': 5, 'Suicidal': 6}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-463a2c8d7e12>:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_input_ids_tensor = torch.tensor(train_input_ids, dtype=torch.long)\n",
            "<ipython-input-22-463a2c8d7e12>:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_attention_mask_tensor = torch.tensor(train_attention_mask, dtype=torch.long)\n",
            "<ipython-input-22-463a2c8d7e12>:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_input_ids_tensor = torch.tensor(test_input_ids, dtype=torch.long)\n",
            "<ipython-input-22-463a2c8d7e12>:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_attention_mask_tensor = torch.tensor(test_attention_mask, dtype=torch.long)\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TRAINING BERT\n",
        "\n",
        "from torch.optim import AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# defining the optimizer and the learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# the t otal number of training steps = number of batches * number of epochs\n",
        "epochs = 3\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# learning rate scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=0,\n",
        "                                            num_training_steps=total_steps)\n",
        "\n",
        "#CrossEntroyLoss measures model's predictions vs true labels (accuracy)\n",
        "loss_fn = CrossEntropyLoss()\n",
        "\n",
        "\n",
        "#checks if GPU is available, otherwise uses a CPU *moves model to whichever one is available\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    #tracks model's performance for each batch\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        # unpack the batch\n",
        "        input_ids_batch, attention_mask_batch, labels_batch = [b.to(device) for b in batch]\n",
        "\n",
        "        # clear previous calculated gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        #forward pass (with automatic gradient calculation)\n",
        "        #for a given batch, unpacks it into 3 part and moves it to either GPU/CPU\n",
        "        outputs = model(input_ids=input_ids_batch, attention_mask=attention_mask_batch, labels=labels_batch)\n",
        "        #convert output items from long to float\n",
        "        target = [0,1,2,3,4,5]\n",
        "        loss = loss_fn(outputs.logits, labels_batch)\n",
        "\n",
        "        # fackward pass\n",
        "        # compares the model's predicted label classification to the real given label & calculates loss (accuracy)\n",
        "        # computes gradient of loss (change of loss with respect to change in paramter *input data)\n",
        "        loss.backward()\n",
        "\n",
        "        # clips the gradients to prevent exploding gradients\n",
        "        # caps/limits graident scale to max value of 1.0\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # adjusts learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        # Track loss\n",
        "        #adds current batch loss to the total for each epoch\n",
        "        total_loss += loss.item()\n",
        "        #print(\"batch\")\n",
        "\n",
        "   #prints avg loss at end of each epoch\n",
        "   #provides insight into how well the model is learning over time\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f'Epoch {epoch + 1}, Loss: {avg_loss}')\n",
        "\n",
        "    #TESTING\n",
        "    # sets model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    with torch.no_grad(): #disable gradient calculation during evaluation\n",
        "        for batch in test_dataloader:\n",
        "            input_ids_batch, attention_mask_batch, labels_batch = [b.to(device) for b in batch]\n",
        "\n",
        "            #forward pass\n",
        "            outputs = model(input_ids=input_ids_batch, attention_mask=attention_mask_batch)\n",
        "\n",
        "            # get the predicted labels\n",
        "            predicted_labels = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "            # calculate accuracy\n",
        "            correct_predictions += (predicted_labels == labels_batch).sum().item()\n",
        "            total_predictions += labels_batch.size(0)\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    print(f'Accuracy: {accuracy:.2f}')\n",
        "\n",
        "    # saving the model to a directory!!\n",
        "    model_save_path = '/content/drive/My Drive/mood_guard'\n",
        "    model.save_pretrained(model_save_path)\n",
        "    tokenizer.save_pretrained(model_save_path)\n",
        "    print(f'Model saved to: {model_save_path}')\n"
      ],
      "metadata": {
        "id": "0MqStY1cJ3hT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62eaf411-f25d-4c0a-eaf2-a5d6cdb415ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.9580651595889325\n",
            "Accuracy: 0.75\n",
            "Model saved to: /content/drive/My Drive/mood_guard\n",
            "Epoch 2, Loss: 0.5391891321025221\n",
            "Accuracy: 0.79\n",
            "Model saved to: /content/drive/My Drive/mood_guard\n",
            "Epoch 3, Loss: 0.41204572603645095\n",
            "Accuracy: 0.80\n",
            "Model saved to: /content/drive/My Drive/mood_guard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#IMPLEMENTATION / TESTING MODEL WITH USER INPUT\n",
        "\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "\n",
        "# loading the model and tokenizer from the specified path\n",
        "model = BertForSequenceClassification.from_pretrained('/content/drive/My Drive/mood_guard')\n",
        "tokenizer = BertTokenizer.from_pretrained('/content/drive/My Drive/mood_guard')\n",
        "\n",
        "sample_text = \"I'm very depressed\"\n",
        "\n",
        "# preprocessing the input text\n",
        "inputs = tokenizer(sample_text,\n",
        "                   padding=True,            # Pads to the maximum length in batch\n",
        "                   truncation=True,         # Truncates if too long\n",
        "                   max_length=512,          # Maximum length for BERT\n",
        "                   return_tensors='pt')     # Return PyTorch tensors\n",
        "\n",
        "# make predictions\n",
        "model.eval()  # this sets the model to evaluation mode\n",
        "with torch.no_grad():  # disables gradient calculation\n",
        "    outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
        "    logits = outputs.logits  # get the logits from the model's output\n",
        "\n",
        "# converts logits to probabilities stats using softmax\n",
        "probs = torch.softmax(logits, dim=1)  # Apply softmax to get probabilities\n",
        "\n",
        "# getting the predicted class\n",
        "# gets the index of the max probability\n",
        "predicted_class = torch.argmax(probs, dim=1).item()\n",
        "\n",
        "# mapping the predicted class index back to the original label\n",
        "label_mapping = {0: 'Anxiety', 1: 'Bipolar', 2: 'Depression', 3: 'Normal', 4: 'Personality Disorder', 5: 'Stress', 6: 'Suicidal'}  # Example mapping\n",
        "\n",
        "\n",
        "predicted_label = label_mapping[predicted_class]\n",
        "\n",
        "# output the result\n",
        "print(f'The predicted category for the input text is: {predicted_label}')\n",
        "\n"
      ],
      "metadata": {
        "id": "gdceGnrGyyCn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ca4f47f-2b2a-4537-ce16-bfb7786bbb1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The predicted category for the input text is: Anxiety\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "mnGzyceMGgR8",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f8eeff1-65f5-4681-8df2-e31394794815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.44.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.3.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.7)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.4)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.12)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.6.8)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.3)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.31.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (12.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi<1.0->gradio) (0.38.6)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#GUI\n",
        "import gradio as gr\n",
        "\n",
        "from __future__ import annotations\n",
        "from typing import Iterable\n",
        "import gradio as gr\n",
        "from gradio.themes.base import Base\n",
        "from gradio.themes.utils import colors, fonts, sizes\n",
        "import time\n",
        "\n",
        "my_theme = gr.themes.Monochrome(\n",
        "    primary_hue=\"cyan\",\n",
        "    secondary_hue=\"teal\",\n",
        "    neutral_hue=\"teal\",\n",
        ").set(\n",
        "    body_background_fill='*color_accent_soft',\n",
        "    background_fill_primary='*primary_50',\n",
        "    border_color_primary='*neutral_700'\n",
        ")\n",
        "\n",
        "#puts pop up on the screen\n",
        "def info_fn(user_input):\n",
        "    response = call_model(user_input)\n",
        "    #fetch the specific message for the predicted category\n",
        "    message = category_messages.get(response, \"Unable to determine a category. Please try again.\")\n",
        "    return gr.Info(message)\n",
        "\n",
        "# dictionary: mapping each category to a specific message\n",
        "category_messages = {\n",
        "    \"Anxiety\": \"It looks like you're feeling anxious. Consider practicing relaxation techniques. You can also call or text a NAMI representative for free at 800-950-6264. <a href='https://open.spotify.com/playlist/3l6b0zuXjgyPxLK6PIAqED?si=0ywZftzXQJmGAJyNGwaxUA'>Check out this Spotify playlist for more support.<a/>\",\n",
        "    \"Bipolar\": \"Managing bipolar disorder can be challenging. Keeping a regular routine can help. You can contact the Depression and Bipolar Support Alliance at 312-642-0049. <a href='https://open.spotify.com/playlist/1UXZQs7qhBMnFYO00QxQtd?si=aJP_VostSV-z3kOYRxrt9Q&pi=u-17BWOZ9ZSOuI&nd=1&dlsi=5a70ab4d0f9d4538'>Check out this Spotify playlist for more support.<a/>\",\n",
        "    \"Depression\": \"It seems you're feeling depressed. Talking to a counselor or therapist might help. You can contact the Depression and Bipolar Support Alliance at 312-642-0049. <a href='https://open.spotify.com/playlist/7INcD4lmarWTQiDVodjVt4?si=2cS5G230REqV3kD2MA28nQ&pi=u-2PmAr4bFQn2X&nd=1&dlsi=b7742602f12d4430'>Check out this Spotify playlist for more support.<a/>\",\n",
        "    \"Normal\": \"Make sure to keep taking care of your mental health! If you want to help others in need, you can volunteer at Lifeline, CHMA, or CASA! <a href='https://open.spotify.com/playlist/6ghnEF33uY7YCgnum8NWAR?si=QnPO4vo3SbGI0BRlRgfiRA&pi=u-VxAETTCLSU60&nd=1&dlsi=5f5516b9657a4385'>Check out this Spotify playlist for more support.<a/>\",\n",
        "    \"Stress\": \"You might be feeling stressed. Try mindfulness or take short breaks to relax. You can also contact the National Mental Health Hotline for free at 866-903-3787. <a href='https://open.spotify.com/playlist/6Ttw2Vt8F5zRqYSb4qcmTo?si=FUjz5r_QTrSPsmCcT1jNhw&pi=u-1pjEp6QjS4O-&nd=1&dlsi=c674a614103b46b7'>Check out this Spotify playlist for more support.<a/>\",\n",
        "    \"Suicidal\": \"Please seek immediate help from a trusted person or a helpline. You can contact the Suicide Lifeline at 1-800-273-8255. <a href='https://open.spotify.com/playlist/7INcD4lmarWTQiDVodjVt4?si=2cS5G230REqV3kD2MA28nQ&pi=u-2PmAr4bFQn2X&nd=1&dlsi=8bb79cc179ef46af'>Check out this Spotify playlist for more support.<a/>\"\n",
        "}\n",
        "\n",
        "#passes the user input to the model so it can classify what the user is feeling\n",
        "def call_model(user_input):\n",
        "    from transformers import BertForSequenceClassification, BertTokenizer\n",
        "\n",
        "    #loads the model and tokenizer from the specified path\n",
        "    model = BertForSequenceClassification.from_pretrained('/content/drive/My Drive/mood_guard')\n",
        "    tokenizer = BertTokenizer.from_pretrained('/content/drive/My Drive/mood_guard')\n",
        "    #preprocess the input text\n",
        "    inputs = tokenizer(user_input,\n",
        "                      padding=True,            # Pads to the maximum length in batch\n",
        "                      truncation=True,         # Truncates if too long\n",
        "                      max_length=512,          # Maximum length for BERT\n",
        "                      return_tensors='pt')     # Return PyTorch tensors\n",
        "\n",
        "    #make predictions\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
        "        logits = outputs.logits  # Get the logits from the model's output\n",
        "\n",
        "    #convert logits to probabilities using softmax\n",
        "    probs = torch.softmax(logits, dim=1)  # Apply softmax to get probabilities\n",
        "\n",
        "    #get the predicted class\n",
        "    predicted_class = torch.argmax(probs, dim=1).item()  # Get the index of the max probability\n",
        "\n",
        "    #map the predicted class index back to the original label\n",
        "    label_mapping = {0: 'Anxiety', 1: 'Bipolar', 2: 'Depression', 3: 'Normal', 4: 'Personality Disorder', 5: 'Stress', 6: 'Suicidal'}  # Example mapping\n",
        "\n",
        "\n",
        "    predicted_label = label_mapping[predicted_class]\n",
        "\n",
        "    #output the result\n",
        "    return predicted_label\n",
        "\n",
        "\n",
        "with gr.Blocks(theme=my_theme) as demo:\n",
        "    gr.Markdown(\"### MoodGuard\")\n",
        "\n",
        "    #textbox for user input\n",
        "    user_input = gr.Textbox(label=\"Enter a message\")\n",
        "\n",
        "    #submit button\n",
        "    submit_button = gr.Button(\"Submit\")\n",
        "\n",
        "    #sets up the button click event\n",
        "    submit_button.click(info_fn, inputs=user_input)\n",
        "\n",
        "\n",
        "#iface = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n",
        "#iface.launch()\n",
        "if __name__ == \"__main__\":\n",
        "  demo.launch()"
      ],
      "metadata": {
        "id": "ld2dk_iLBzeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "outputId": "328a2edc-bae8-4a21-b53e-0f818d1fadc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://05c3e6fc19daffc4c3.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://05c3e6fc19daffc4c3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}